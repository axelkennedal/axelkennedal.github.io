---
title: An AI Intelligence Explosion - More Possible Than You Think
---
# Introduction
In [an article published on Medium.com](https://medium.com/@francois.chollet/the-impossibility-of-intelligence-explosion-5be4a9eda6ec) François Chollet argues that an *Intelligence Explosion* is impossible. First of all, what is an Intelligence Explosion? Here is how I.J Good in 1965 was the first one to describe an intelligence explosion:

> Let an ultraintelligent machine be defined as a machine that can far surpass all the intellectual activities of any man however clever. Since the design of machines is one of these intellectual activities, an ultraintelligent machine could design even better machines; there would then unquestionably be an “intelligence explosion,” and the intelligence of man would be left far behind. Thus the first ultraintelligent machine is the last invention that man need ever make, provided that the machine is docile enough to tell us how to keep it under control.

Now, I've summarized Chollet's arguments into points (though I recommend reading his article too in order to get the full picture).

## Arguments
i. Intelligence is highly dependent on its surroundings - your environment and human culture is a fundamental part of your mind where all of your thoughts come from. Humans growing up in the wild become effectively animals and don’t develop human intelligence. They also have problems trying to adapt to civilization later in life. The bottleneck is our circumstances. You cannot put a human brain in an octopus body and expect it to survive and take control of its body.

ii. There is no such thing as general intelligence - no problem-solving algoritm can outperform random chance across all possible problems; problem-solving algorithms can only focus on very specific problems (such as image recognition and playing Go). Extraordinarily intelligent humans don’t often accomplish much of importance, more often it’s humans with regular intelligence, who happened to be at the right place at the right time with the right specialization.

iii. Intelligence is not just in our brain, it’s in our civilization - in books, other people, Google search etc.

iv. A human brain is not capable of designing a greater intelligence than itself - the argument here is that it is not possible because it hasn’t yet happened. He then goes on to say that a civilization could design superhuman intelligence, but this superhuman intelligence could not design an intelligence greater than itself, reusing the argument that no-one has done it so far.

v. There are already recursively self-improving systems, and they do not have superhuman intelligence. You are yourself a recursively self-improving system; educating yourself makes you smarter, allowing you to educate yourself more efficiently. Human civilization is recursively self-improving on a large timescale. Our computers are only incrementally more useful to us than they were in 1992. Science itself is also a self-improving system but is only reaching linear improvement. He concludes this point stating that these systems are orders of magnitude simpler than a self-improving mind.

# Counterarguments

I will now argue why Chollet is wrong on all points - why these arguments do not make an intelligence explosion impossible. To start with I’d like to point out that the description of an intelligence explosion does not mention anything about exponential growth (which you'll encounter Chollet mentioning many times in his article). So what Chollet argues is then that there could not be an ultra-intelligent machine self-improving to become smarter than humans. Now allow me to move on to tackling one argument at a time.

In his first point he states that intelligence is highly dependent on its surroundings, that all of our thoughts come from our environment. I agree that without any input to a human brain it would not have many thoughts. Similarly, a computer will not compute very much or interesting things without any input. But that doesn’t really matter when it comes to whether an intelligence explosion is possible or not. When humans work towards designing an ultra-intelligent machine they will provide it with inputs in order for it to learn about the world. Because if the AI has a goal (or several goals) pertaining to the physical universe (such as making sure all humans are fed, and that there is no poverty) it will want to know more about the physical universe. What is very likely to happen then is that just like us humans developed tools like telescopes, microscopes and all sorts of sensors in order to get more input to our brains about the physical universe, the AI will develop and put to use more sensors for itself, gaining more and more data about the universe. The only limit here to what an AI could achieve is based on the laws of physics - how fast it can interact with and pull data from its sensors, how fast it can compute and process the input and how fast it can rearrange matter. Regarding putting a human brain in an octopus body; it is true that the human brain is specialized in some sense at controlling human limbs, not octopus limbs, so that would likely not go down very well. But we are not discussing human brains, we are talking about some form of general machine intelligence, which is quite different. It turns out that an AI can actually learn how to control limbs it is given, just by trial and error (repeated many, many times): In 2017 Google DeepMind gave an AI a virtual body with sensors, and [the AI learned how to control the body and walk in a virtual environment](https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/) (given a goal, of going from point A to B). It was never even shown what walking looks like.

Moving on to his second point - ”general intelligence doesn’t exist”. It is true that problem-solving algorithms are specific to the problem one is trying to solve. But the problem with this argument is an implicit assumption that the AI would be an algorithm which could more or less instantly solve any given problem with no training or beforehand knowledge of the problem. This is of course not how a sophisticated AI works. How do you learn to tackle a new problem? I can almost guarantee you that you first try a slightly random approach (perhaps influenced a bit by previous similar problems you’ve solved). After trying randomly for a while you would be able to find patterns and use those to change and improve how you solve the problem. A machine could easily do this with reinforcement learning. So once the AI encountered a new problem it could run millions of simulations to find out how problems like it are solved, then add this new ”knowledge” or ”skill” to its software. The fact that many famous scientists and distinguished humans do not have a high IQ is not unexpected in my opinion. As Chollet writes himself ”there are currently about seven million people with IQs higher than 150 - better cognitive ability than 99.9% of humanity - and mostly, these are not the people you read about in the news”. If 99.9% of humanity is in this ”not super-intelligent” group, then it seems quite statistically likely that people from this group will end up in the news and become renowned scientists, don’t you think? Especially since IQ does not measure how much knowledge and experience someone has within a field (e.g. biology or physics), and since it’s in general debatable whether IQ really is a good measurement of intelligence.

”Intelligence is not just in our brain, it’s in our civilization - in books, other people, Google search etc.”. This statement is just purely false. Chollet is confusing information with intelligence. Physical artefacts representing information that humans have gathered are not intelligent.

In Chollet’s fourth argument he states that a human brain cannot design an intelligence greater than itself. His only argument for this is that it hasn’t yet happened. Humans flying hadn’t happened before the first air craft was invented - it seemed impossible before it had happened - but then it happened! I’m sure you can come up with plenty of examples like this. Saying that something cannot happen at all because it hasn’t yet happened is a terribly bad argument and highly unscientific - a real scientist knows that nothing is set in stone, new findings could at any time disprove previously accepted theories and bring forth new ones.

Finally we arrive at his last argument, that there already are self-improving systems - and they have not achieved super-intelligence. His conclusion here is that these existing systems are orders of magnitude simpler than a self-improving mind. My question then is how he could possible say that a system which is an order of magnitudes more complex than the existing ones would not allow for super-intelligence. If anything, the fact that it is so much more complex speaks for the fact that it *would* be able to achieve super-intelligence.

# Conclusion
I consider all of Chollet’s arguments disproven - an intelligence explosion is possible, and will in my opinion likely happen within this century. Do you agree or disagree? Let me know your opinion in the comments.
